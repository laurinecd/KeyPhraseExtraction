{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bee6ac6",
   "metadata": {},
   "source": [
    "1. Download `Krapivin2009` dataset with `git clone https://github.com/INESCTEC/KeywordExtractor-Datasets.git`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df21c1",
   "metadata": {},
   "source": [
    "2. Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "481c808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1005058',\n",
       " 'text': '--T\\nEnhancing Product Recommender Systems on Sparse Binary Data.\\n--A\\nCommercial recommender systems use various data mining techniques to make appropriate recommendations to users during online, real-time sessions. Published algorithms focus more on the discrete user ratings instead of binary results, which hampers their predictive capabilities when usage data is sparse. The system proposed in this paper, e-VZpro, is an association mining-based recommender tool designed to overcome these problems through a two-phase approach. In the first phase, batches of customer historical data are analyzed through association mining in order to determine the association rules for the second phase. During the second phase, a scoring algorithm is used to rank the recommendations online for the customer. The second phase differs from the traditional approach and an empirical comparison between the methods used in e-VZpro and other collaborative filtering methods including dependency networks, item-based, and association mining is provided in this paper. This comparison evaluates the algorithms used in each of the above methods using two internal customer datasets and a benchmark dataset. The results of this comparison clearly show that e-VZpro performs well compared to dependency networks and association mining. In general, item-based algorithms with cosine similarity measures have the best performance.\\n--B\\nIntroduction\\nCustomers on the web are often overwhelmed with options and flooded with promotional messages for\\nproducts or services they neither need nor want. When users cannot find what they are searching for, the\\ne-commerce site struggles to maintain good customer relations.\\nEmploying a recommender system as part of a site\\'s Customer Relationship Management (CRM) activities\\ncan overcome the problems associated with providing users with too little information, or too much of\\nthe wrong information. Recommender systems are able to assist customers during catalog browsing and are\\nan e#ective way to cross-sell and improve customer loyalty.\\nIn this paper, we will compare several recommender systems being used as an essential component of\\nCRM tools under development at Verizon. Our solutions are purposely for the current customers and current\\nproducts - recommendations for new customers and new products are out of the scope of this paper.\\nAccording to [Breese et al., 1998], there are two main approaches used in recommender systems: memory\\nand model-based systems. Memory-based recommender systems, such as correlation analysis and vector\\nsimilarity, search the customer database for customer profiles that are similar to the profile of the active\\ncustomer. For this type of recommender system, it is important that the customer database remain in system\\nmemory during the algorithm\\'s runtime. Model-based methods, such as Bayesian networks and Clustering\\nmodels, approach the problem from a probabilistic perspective to find the best product for a given customer\\nprofile [Breese et al., 1998] and need only keep the resulting model in memory while the algorithm runs. The\\nabove methods will be described in the following sections.\\nBecause memory-based approaches make predictions based on the local neighborhood of the active user\\nand the model-based approach bases its predictions on the similarities between items (products), recommender\\nsystems can be grouped into user-based and item-based systems [Sarwar et al., 2001, Sarwar et al., 2000].\\nUser-based systems, as the name suggests, use historical information to identify the neighborhood for\\nthe active customer. Products are then recommended according to their similarities to this neighborhood.\\nBecause user-based recommender systems use the customer profile data, they can incorporate demographics\\ndata along with the historical purchase data. In contrast, item-based method use only historical purchase\\ndata to identify similarities between di#erent items. Unfortunately, when the size and the number of dimensions\\nof a customer database are very large, the time required to perform a search utilizing user-based\\nmethods may become prohibitive.\\nNeighborhoods in user-based algorithms are defined according to several measures. Two of these measures\\nare used frequently: Pearson\\'s correlation coe#cient and cosine. Pearson\\'s correlation coe#cient is computed\\nbetween the active customer and the rest of the customers in the database. Based on their similarity to\\nthe active customer, k customers are selected. Statistics regarding this neighborhood are calculated later in\\norder to accurately recommend products. Pearson\\'s correlation coe#cient between customer a and customer\\nb is computed using the following formula:\\nu a )(u bi -\\n(1)\\nwhere u ai corresponds to the a th row and i th column of the user-item matrix and u ai is equal to 1 if the\\ncustomer has the i th product or 0 otherwise. These binary values are equivalent to the user ratings (votes)\\nin the discrete valued data. An average vote for the customer a in this instance is indicated as -\\nu a . It is also\\npossible to form the cosine between two customers (a and b) in neighborhood k using the following equation:\\n(2)\\nThe similarity measures defined in Equations 1 and 2 can also be used in item-based recommender\\nsystems. In this case, similarities between the items are found first. During the prediction phase, the items\\nthat are most similar to the items found in the active user\\'s profile are recommended. Because of the nature\\nof the binary user-item data, these similarity measures may perform poorly for prediction purposes. Several\\nmodifications to these measures are proposed in [Breese et al., 1998] to improve the performance of user-based\\nrecommender systems. In this paper, both correlation and cosine-based similarity measures are used\\nin the context of item-based recommender systems.\\nDependency networks are used to increase the e#ciency of collaborative filtering. Each node in a dependency\\nnetwork represents an item (product). In this type of modeling, items missing from the customer\\nprofile data are also considered, so that the models found by a dependency network will include some items\\nthat a customer does not have. Dependency network modeling is available with Analysis Services of MS SQL\\nServer 2000 and is also hard-coded into MS Commerce Server 2000 as the standard product recommendation\\nsystem. The dependency network model is discussed in Section 2.2.\\nAssociation mining models have been successfully used in product recommendation systems. In terms of\\nthe memory and model-based taxonomy, association mining does not fit easily into either group. Finding\\nassociation rules based on historical data is similar to the methodology of the model-based approach, but\\nthe size and exact search mechanisms between customer profile and association rules resemble item-based\\napproaches. In this manner, it is better to classify association mining as an item-based approach with higher\\nmemory requirements. Our proposed method, e-VZpro, does not use the exact match between active user\\nprofile and association rules, but instead searches for those rules that are the most similar with the higher\\nconfidence levels.\\nWhen implementing a recommender system, there are two important issues to consider: the scalability of\\nthe filtering algorithm and the quality of the recommendations [Sarwar et al., 2001]. Successful e-commerce\\nsites may have several million customers and product catalogs that include thousands of items. When\\neach customer is interested in and purchases only a small number of items, the user-item matrix used by\\ncollaborative filtering algorithms may be very sparse - which, in turn, produces poor recommendations for\\ncustomers. To increase the quality of recommendations, a user-based collaborative filtering algorithm may\\nbe introduced, which compromises scalability. It can be seen, then, that quality of recommendations and\\nneed for scalability in a collaborative filtering system are often in conflict with each other.\\nThis paper seeks to make four major contributions. We use a similarity measure for finding the best rule\\ninstead of an exact match. We show that recommender systems based on association mining have stronger\\npredictive abilities when compared to dependency network systems. We also demonstrate how clustering\\ndata improves the predictive qualities of association rules. Finally, we conduct an empirical study to compare\\nseveral algorithms on very sparse and binary user-item data.\\nThe paper is organized as follows: Section 2 contains a brief discussion of some related work and\\nother recommender systems. The methodology of our approach is discussed in Section 3, along with a\\ntoy example. There are two internal Verizon datasets and the MSWeb benchmark dataset (provided in\\n[Hettich and Bay, 1999, Breese et al., 1998, Heckerman et al., 2000]) for empirical comparisons of various\\nrecommender systems. The summary of the study\\'s findings is reported in Section 4, along with details of\\nour implementation. Section 5 contains our conclusions and information on future work directions.\\nRelated Work\\nThe introduction of customer-centric management techniques and the emergence of a web-based economy\\nhave forced the creation of a new generation of companies and the evolution of existing, traditional companies.\\nThe deregulation of previously protected industries has also led to increased competition for many companies,\\nwho now seek to improve their service quality and customer loyalty. Companies in every industry are finding\\nthat their customers increasingly demand products, services, and solutions that are tailored to their specific\\nneeds.\\nRapid changes in internet technologies, especially the increasing availability of high-speed broadband\\nconnections, allow customers to go online and easily research the products and services they desire. Customers\\nare able to compare competing items through corporate web sites, so much so that e-commerce sites\\nnow play a role as important as the traditional customer touch-points such as call centers.\\nIn every contact with a customer, companies need to o#er their best products and services to gain\\ncustomer confidence and retain consumer loyalty. The automated nature and frequent use of e-commerce\\nsites highlights the need for recommender systems that can assist the customer. But recommender systems\\nare also useful in call centers, where the sta# is not always as experienced or trained in sales as would be ideal\\nand the system could assist the customer service and sales representatives. In these cases, a recommender\\nsystem should be designed in accordance with the company\\'s overall marketing strategy and a successful\\nsystem will also benefit employees by helping them attain larger performance bonuses.\\n2.1 Common Recommender Systems\\nIn the past decade, recommender systems have been put to successful use in the publishing, news, film, music,\\nand video industries. Although \"collaborative filtering\" was first coined in [Goldberg et al., 1992], communications\\nof the ACM featured several pioneering works in the third issue of Volume 40 [Resnick and Varian, 1997]\\nin 1997, including the GroupLens Project [Konstan et al., 1997], and Siteseer [Rucker and Polanco, 1997].\\nGroupLens Project is a recommender system based on the Usenet news groups wherein recommendations\\nare made according to the correlations among the news ratings provided by the users. Depending on the\\nuser\\'s ratings, he/she gets recommendations of the unread messages and these unread messages can then be\\nsorted according to their predicted rankings. Although news groups are clustered well in terms of the content\\nand the user preferences, each user can only read and evaluate (rate) very few messages, which causes an\\nobvious sparsity problem for predictions.\\nSiteseer, on the other hand, recommends web pages based on bookmark similarities, eliminating the need\\nfor user ratings. Early recommender systems had the advantage of working with communities wherein the\\nusers are very similar to one another, as is the case with newsgroups and other internet-based communities.\\nA recent work by Shafer, Konstan, and Riedl [Schafer et al., 2001] summarizes the taxonomy of recommender\\nsystems and their use in e-commerce applications. Recommender systems have the potential for\\ngreater predictive ability than database marketing techniques, since recommendations - depending on the\\nmethod used - are made in highly similar, homogenous, and small neighborhoods rather than in segments\\nconsisting of much larger populations.\\nThe recommender systems from six e-commerce sites, including Amazon.com, CDNOW, Drugstore.com,\\neBay, MovieFinders.com, and Reel.com are given in [Schafer et al., 2001] as examples of the currently popular\\nrecommender systems. These e-commerce sites use a number of recommendations, such as Customers\\nWho Bought, Customers\\' Ratings, Editors\\' Picks, Top 100, and Customer Preferences. An important consideration\\nwhen deploying a recommender system is the method of recommendation delivery. An online,\\nreal-time solution requires an extremely fast response from the system, but recommendations delivered via\\ne-mail do not require great speed, but must insure good quality.\\nThe \"keyword search\" interfaces commonly used on e-commerce sites can be considered one of the simplest\\ntypes of recommender systems. Virtually all e-commerce sites have this feature by default, allowing\\nany customer to search for information related to specific products or product categories. High volume,\\nstandardized products or highly recognized brands are the most likely to see significant benefits from using\\na simple search utility as a recommender system.\\nRecommender systems based around \"editors\\' picks\\'\\' are also very common. These systems require\\n\"manual rules\" for their product recommendations and the expert opinions are very useful in extremely\\nconnected communities with very similar member preferences.\\nA recommender system based on general public opinion is also useful in many domains. Basic statistical\\nsummaries such as the most popular products or Top 100 music charts will be su#cient for this purpose.\\nStatistical summaries are best used for non-personalized recommendations [Schafer et al., 2001].\\nAs we briefly mentioned in our discussion of user-based recommender systems in Section 1, the goal of\\nthese recommender systems is to predict the utility of the product o#erings to the active customer based\\non the preferences of either a select group of customers or all customers [Breese et al., 1998]. As given in\\nEquations 1 and 2, the most popular methods for computing similarity are correlation coe#cient and cosine.\\nOf course, the vector similarity is a na-ve way of finding customers similar to the active customer; inverse\\nuser frequency and case amplification are modifications to similarity methods that can be used to reduce\\nthe weights of highly used (preferred) products. In cases where there are only a few votes or purchased\\nitems common among users, similarity measure based on correlation may be problematic. Default voting, to\\nassign minimal weights to some items, was proposed in [Breese et al., 1998] as one method to prevent poor\\nperformance in the course of intersecting common votes (purchased products) and, instead of the intersection,\\nthe union of voted items could be used [Breese et al., 1998].\\nA number of successful e-commerce sites have employed user-based methods with the assumption that\\neach customer or individual belongs to a larger group of customers [Karypis, 2001]. Critics arise from the\\nscalability and online performance of these algorithms and the sparsity of the user-item matrix may require\\nthe use of some of the remedies mentioned above, such as inverse user frequency. On the other hand, the\\nsparseness may result in shortened recommendation times with the implementation of the sparse matrix\\noperations for calculating similarities.\\nItem-based recommender systems are very e#ect when a large customer database is available [Sarwar et al., 2001,\\nKarypis, 2001]. Like user-based recommender systems, cosine and correlation based similarities are used to\\nfind the item similarities. Conditional probability [Karypis, 2001] is another way of computing item similarities\\nand modified conditional probabilities and adjusted cosine similarities [Sarwar et al., 2001] are often used\\nto minimize the e#ect of high-frequency items. Frequency scaling is a common method used in information\\nretrieval systems as well, and remarkably short recommendation times have been reported [Karypis, 2001]\\nwhen using this methodology in item-based recommender systems. In [Karypis, 2001] a regression model\\nwas used to predict the recommendations after finding items similar to the active customer profile, showing\\nthat item-based recommender systems compromise very little on recommendation quality while providing\\nsubstantial gains in computational e#ciency [Sarwar et al., 2001].\\nStatistical and Machine Learning techniques have been successfully used for the model-based recommender\\nsystems. Indeed, collaborative filtering is a major research topic in statistical and machine-learning\\ncommunities. The probabilistic nature of the problem requires robust and e#cient techniques to recommend\\nproducts accurately. In the following subsection, we will discuss a well-known collaborative filtering\\ntechnique based on the dependency networks introduced in [Breese et al., 1998, Heckerman et al., 2000].\\n2.2 Dependency Networks\\nBayesian networks have been successfully used in the visualization of the predictive relationships between\\nthe variables of a particular dataset. There is a problem, however, in that Bayesian networks can be very\\nconfusing for an untrained person to interpret [Heckerman et al., 2000]. Dependency networks are proposed\\nin [Heckerman et al., 2000, Breese et al., 1998] to represent predictive or correlational relationships that\\nexist in data in a more easily comprehensible manner. On the other hand, it is computationally very easy\\nto construct a dependency network compared to the e#ort required to create similar Bayesian networks. In\\naddition, it is a simple task for any probabilistic classification or regression model [Heckerman et al., 2000]\\nto estimate the conditional distribution of each variable in relation to the rest of the variables. Because\\ne#cient probabilistic classification and regression models exist, it takes less time to construct a dependency\\nnetwork than to construct a comparable Bayesian network.\\nTo illustrate the computational e#ciency of the dependency network, we have adopted the following\\nsimple example from [Heckerman et al., 2000]. Assume the problem is to find the dependency networks from\\nthe domain In this case, all that is required is to estimate three conditional probability\\ndistributions These can be estimated using any classification\\nmethod, such as logistic regression, neural networks, and probabilistic decision trees. For illustration pur-\\nposes, assume these methods are capable of variable selection and that one of these methods, let\\'s say the\\nprobabilistic decision trees, found that X 1 is not a significant predictor of X 3 and X 3 is not a signification\\npredictor of X 1 . In this case, we can easily construct the following dependency network:\\nThis example shows how dependency networks are constructed in a very precise and simple manner. Note\\nthat the variable selection determines the structure of the dependency network [Heckerman et al., 2000].\\nThough constructing the dependency network is computationally very reasonable in general, inconsistent\\nconditional probability distributions - due to the heuristic search and finite-data e#ects - are potentially major\\ndrawbacks to this method [Heckerman et al., 2000]. Nevertheless, having large datasets in this particular\\napplication gives us the confidence to apply a collaborative filtering based on the dependency networks,\\nsince strong inconsistencies will be rare when dealing with large datasets. Thus, we expect that resulting\\ndependency networks will be \"almost\" consistent according to the discussion in [Heckerman et al., 2000].\\nAnother possible issue is that, in some cases, it is prohibitive to construct dependency networks like other\\nmodel-based methods. Moreover, each additional data requires full training as in the case of many model-based\\nmethods. In the following subsection, we will briefly discuss some of the other model-based approaches.\\n2.3 Other Model-Based Recommender Systems\\nA very early work on recommender systems based on machine learning algorithms is introduced in [Billsus and Pazzani, 1998].\\nThe concept is to first manipulate the data by transforming the user-item matrix into Boolean feature vectors\\nin order to use Singular Value Decomposition to find important features. Finally, a neural network is\\ntrained based on these important features - unfortunately, this method lacks the scalability o#ered by other\\nmethodologies.\\nClustering is successfully used to summarize and analyze data in many di#erent domains. As a natural\\nextension, it has been used in various recommender systems [Ungar and Foster, 1998, Breese et al., 1998] by\\ngrouping customers with similar profiles into the same cluster and then recommending products based on the\\npopularity of each product within that cluster. Expectation-Maximization (EM) based clustering algorithms\\nare used in both [Ungar and Foster, 1998, Breese et al., 1998] of the above methods. The Bayesian Clustering\\nmethod performed very well in terms of accuracy in [Breese et al., 1998] as an alternative to the dependency\\nnetworks and user-based methods.\\nA hybrid method based on Personality Diagnosis (PD) is proposed in [Pennock et al., 2000]. In this\\nmethod, the personality type of the active user is determined and used to compute the probability of\\nthe active user having new items. PD-based collaborative filtering requires using all of the available data\\nthroughout the process, though new data can be added incrementally [Pennock et al., 2000] to adjust model\\nparameters.\\nAssociation mining is best used to perform traditional market basket analysis where the rule discovery is\\nconducted to uncover the associations between various products. Recommender systems based on association\\nmining are very common in e-commerce applications [Sarwar et al., 2000]. The implementation in this paper,\\nhowever, di#ers from previous approaches in an important point - rather than finding exact matches between\\nthe active user and the rule base, e-VZpro finds the most similar rule with the highest confidence rate by\\nscoring each rule in terms both its similarity to the active user and its confidence rate. Association mining can\\nalso be used to predict certain phenomenon (classification problem); a case study in the telecommunication\\nindustry [Ali et al., 1997] uses association mining to predict whether manual assistance will be required for\\na specific order. Based on the Universal Service Order Codes (USOCs) attached to an order, a prediction\\nmodel determines the probability of the need for manual assistance with that order. In the following section,\\nwe will discuss our project methodologies in detail.\\n3 Methodology Used in e-VZpro\\nThere are two phases to our methodology. In the first phase, association mining is used to discover the rules\\nthat exist in historical customer data in a manner similar to that used in item-based recommender systems.\\nDuring the second phase, a scoring algorithm searches the rule base according to the active customer profile\\nto find products that can be o#ered to that customer. Since all rules are kept in memory and a subset of the\\nrules are scored sequentially, this phase is most similar to the user-based recommender systems discussed\\nIn the second phase, each product that the customer does not have is scored by finding corresponding\\nrules for the active product. Scores for the active product are then computed by multiplying the similarity\\nmeasure between the rules and the active customer profile rate by the confidence rate of the rules. More\\nformal scores are computed using the following equation:\\nIn Equation 3, u and r represent the active user and the rule, respectively. The similarity measure is\\ncomputed based on a normalized Euclidian distance measure given in Equation 4. The rationale behind\\nthis formulation is as follows:Euclidian distance is a measure of dissimilarity and, in order to have similarity\\nmeasures between 0 and 1, it is necessary to normalize Euclidian distance by dividing it by the maximum\\ndiscrepancy and then subtracting this normalized distance from 1. Because some products might not be\\navailable in certain areas, e-VZpro was designed to handle these cases by including an option in the customer\\nprofile for \"-1\" to represent an unavailable product in addition to the standard entries of \"0\" and \"1\". Thus,\\nthe maximum discrepancy might be equal to 4# i:r i >0 r i . Where a perfect match between customer profile\\nand rule are found, the similarity is equal to 1. Again, u and r represent the active user and the rule\\nrespectively in Equation 4.\\nBecause the rules and customer profiles are composed of 0\\'s and 1\\'s it is problematic to use correlation\\ncoe#cient and cosine. Because the rules are sparser than customer profiles, using correlation coe#cient or\\ncosine as a similarity measure is misleading. As the algorithm tries to find rules that are similar to the\\nactive user profile, the similarity measure between a rule and the active customer profile is dependent on\\nthe magnitude of the left-hand side of the rule. Association rules might have multiple items on the right\\nhand side of the rules but, due to the nature of the prediction problem in this paper (recommendations are\\nindependent of one another and customers will select only one of several recommendations) we only use rules\\n\\nTable\\n\\n1: Sample Customer Data\\nCust-id USOC\\nthat have singleton right-hand sides.\\nThe highest score for o#ered products determines the ranking of each product. In addition, revenue\\ninformation (recurring charges) can be used to weight the scores to find the final ranking of each product -\\nthis method is not explored in this paper. Finally, the Top-N products as determined above are o#ered to\\nthe active user. While scoring is done on the fly, association mining is a batch process and, thus, e-VZpro\\nclearly resembles both the memory-based and user-based methods. In the rest of this section, we will explain\\nour methodology with a toy example.\\nGiven the customer purchase data (such as in Table 1), association mining, also known as market basket\\nanalysis, discovers the rules (relationships) that exist within the historical customer purchase (service order)\\ndata. Each of the rules include \"if\" clauses by default and the structure of the rules is as follows: If the\\ncustomer purchases Product A, then with C% probability he/she will buy Product B. This probability is\\ncalled the confidence rate. More formally, the confidence rate can be computed as:\\nwhere\\nA#B indicates transactions that include both Product A and Product B. Confidence rate is also equivalent\\nto the conditional probability of a customer having Product B if they already have Product A. Another\\nimportant statistic in association mining is the support level, which is essentially equal to the fraction of\\ntransactions that have both Product A and Product B. Thus, the support level S can be computed as:\\nwhere T is equal to the total number of transactions. The support level is the most\\nimportant factor when pruning the association rule base.\\nTo depict our analysis with a simple example, a toy problem is provided in Table 1. USOCs given in\\n\\nTable\\n\\nare actual values extracted from our databases. The descriptions of the USOCs are found in Table\\n2.\\nAfter the analysis with 100% confidence rate, we found the following rules using association mining:\\n\\nTable\\n\\n2: Descriptions of USOCs used in the Sample Data\\nUSOC Description\\n41368 Personal 800 Number\\n42267 Basic Voice Mail Service\\n47036 Caller ID -Name and Number\\n55822 Non-published Listing\\nFor example, the first rule implies that if the customer has Caller ID - Name and Number, then it is 100%\\nlikely that he/she will have or by Non-Published Listing. On the other hand, the last rule implies that if\\nthe customer has both Non-published Listing and Basic Voice Mail Service then there is a 100% probability\\nthat he/she will have/buy Caller ID - Name and Number. Assuming that we want to o#er a product to a\\ncustomer with only the Basic Voice Mail Service in his/her profile, we can o#er the customer any or all of\\nthe other three products. Since Basic Voice Mail Service is not found on the left side of any of the first 4\\nrules, they receive the lowest scores. The first two rules have a similarity measure of 1 - # 1\\n0.5. The\\nthird and fourth rules have a similarity measure of 1 - # 1+1\\nBut the last two rules are some how\\nsimilar to the customer profile. Their similarity measures are 1 - # 1+0\\n0.6465. The final scores are also\\nequal to 0.6465 because the confidence rates are equal to 1 for both rules. Finally, we are able to o#er the\\nCaller ID - Name and Number as well as the Non-Published Listing.\\nWe used 100% confidence rating to illustrate the algorithm; while it would have been possible to use a\\nlower confidence rate, it would have required a prohibitive number of rules to illustrate in this toy example.\\nAn empirical study is provided in the next section to compare e-VZpro and dependency networks.\\n4 Experimental Results\\nAs was mentioned in the previous sections, there are many factors that a#ect the performance of a given\\nrecommender system. Previous empirical studies [Breese et al., 1998, Karypis, 2001, Sarwar et al., 2001]\\nindicate that item-based approaches are very scalable and result in very negligible losses in recommendation\\nquality when compared to user-based approaches. Our goal in this section is to demonstrate that association\\nmining-based recommender systems are comparable with the well-known recommender system introduced\\nin [Breese et al., 1998, Heckerman et al., 2000] and other item-based recommender systems.\\nProblems studied in this paper do not include user ratings, as it is simple to determine whether or not\\nusers have a specific product. Thus, the user-item matrix is designed to include only 0-1 entries. It is\\nassumed that all products are available to all customers in this study and that users cannot have multiple\\ninstances of the same product.\\nThree di#erent datasets were used in this study - two of these datasets contain internal Verizon customer\\ninformation and the third is the MSWeb dataset from the UCI KDD archive [Hettich and Bay, 1999]. One\\nof the Verizon customer datasets was sampled from a pool of residential customers located in the western\\nstates (WESTDB2) and the second dataset was sampled from a pool of general business customers located\\nin the state of Texas (GENBUS). The samples were taken from the working telephone numbers (WTN)\\nfrom the two di#erent data sources. In reality, customers can have multiple telephone lines, but the current\\nstudy treats each line as a unique customer. A transaction is defined as a set of products (services) and an\\n\\nTable\\n\\n3: Summary of the Datasets\\nStatistics WESTDB2 GENBUS MSWeb\\nNumber of Distinct USOCs 1789 1602 NA\\nNumber of Transactions\\nNumber of Transactions in Training Data 130379 151921 32711\\nNumber of Transactions in Test Data (|U | > 1) 1 22582 18913 3453\\nAve. Number of Items across All Data 2 3.82 - 2.2 2.85 - 1.6 3.02 - 2.5\\nAve. Number of Items across Training Data 4.21 - 2.1 3.12 - 1.5 3.02 - 2.5\\nAve. Number of Items across Test Data 4.21 - 2.1 3.13 - 2.1 3.95 - 2.5\\nNumber of Items in Data 51 61 294\\nNumber of Nonzero Entries in Training Data 548944 473887 98654\\nSparsity of Training Data 0.9174 0.9489 0.9897\\nNumber of Nonzero Entries in Test Data 98446 84887 13644\\nSparsity of Test Data 0.9145 0.9264 0.9866\\nNotes:|U | is equal to the number of products in a transaction.\\nStandard Deviations are reported after - symbols.\\nassociated customer (WTN). The MSWeb dataset was used in [Breese et al., 1998, Heckerman et al., 2000]\\nand details about this dataset can be found there. We use MSWeb as a benchmark dataset that consists of\\nuser activities on certain Microsoft pages.\\nAs we mentioned in the previous section, the orders in the telecommunication industry are processed at\\nthe USOC level. Many orders contain multiple USOCs and, depending on the geographical location, price,\\ndiscount, and regulations, the same product may have di#erent USOCs. Some USOCs are associated with\\ncertain products - such as Caller ID and Call Waiting - but may have no additional charges at all. On the\\nother hand, certain products may have di#erent features and di#erent pricing. Before performing any type\\nof analysis for this study, the data was prepared by grouping the USOCs into 70 meta-product groups and\\neliminating the most frequent products such as residential and business lines. Some products did not have a\\nhigh enough frequency to be included in the analysis, so the WESTDB2 and GENBUS datasets consist of 51\\nand 61 product groups respectively. The characteristics of the datasets are summarized in Table 3. MSWeb\\noriginally had 5,000 test instances, but we omitted the test cases that had only one page in the user profile.\\nThe sparsity of each dataset is defined as 1 - nonzero entries\\ntotal entries in [Sarwar et al., 2001].\\nThe original WESTDB2 and GENBUS datasets have 174,933 and 205,882 customers (transactions) re-\\nspectively. As with the evaluation of any other machine learning method, we divided our datasets into two\\nsubsets: training and test data. Approximately 85% of the original data was reserved as the training data;\\nthose transactions with at least 2 products in the customer profiles were used as the training data from\\nthe reserved data. The training data was used to construct the dependency networks or to discover the\\nassociation rules used to create the rule base for e-VZpro. The remaining data was used as test data.\\nBecause our focus in this paper is to compare the predictive abilities of two di#erent recommender\\nsystems, we adopted the all-but-1 protocol used in [Breese et al., 1998]. As a significant amount of the test\\ndata is only suitable for the all-but-1 protocol, other test protocols were not used. Some of the products\\nmight be used in the models to predict for other products, but we chose to not o#er such products to the\\ncustomer through the recommender system in this study. For example, Product 26 in our dataset is not\\no#ered by the recommender systems in this study, so the test data contains only those transactions that\\nhave at least 2 products other than Product 26 before one of the products was removed according to the\\nall-but-1 protocol.\\nThe accuracy rate of the systems was defined as number of successes\\nnumber of transactions\\nwhere predicting the removed product\\nfrom the test data within the Top-5 o#ers is defined as a success. The predictive abilities of the two di#erent\\nmethods are evaluated by their accuracy rates on the test datasets. The specific details of the dependency\\nnetworks implementation are given in the following subsection.\\n4.1 Implementing The Dependency Networks\\nConstructing the dependency networks is an important feature of MS SQL Server 2000. Unlike other modeling\\nsoftware, SQL Server allows users to construct multiple prediction models in a single step using the\\nsame dataset. Collaborative filtering is a special implementation of this type of modeling that works very\\nwell and is easy to construct. MS Commerce Server 2000 also comes with a recommender system that uses\\ndependency networks, but is limited by its ability to use only 20,000 cases. It was decided that MS Commerce\\nServer 2000 would not be used in this study due to this limitation.\\nTraining the dependency-networks-based recommender systems took around 48, 74 and 194 minutes\\nof computation for the WESTDB2, GENBUS, and MSWeb datasets respectively. All computations were\\nperformed on a dedicated SQL Server 2000 machine with two Pentium III 1 GHz CPUs and 4 GB of physical\\nmemory. Note that the current data mining models in SQL Server 2000 are not parallel algorithms. The\\nresulting dependency networks are depicted in Figure 1 and Figure 2.\\nAn important visualization feature of dependency networks is that they can be filtered to remove the\\nweaker links [Heckerman et al., 2000] and related products are shown closer to one another. Due to the\\ninadequate frequencies of certain products, dependency networks have no links for some products as seen in\\n\\nFigure\\n\\n1. Dependency networks resulted in 41.93%, 47.59%, and 31.07% accuracy rates for the WESTDB2,\\nGENBUS, and MSWEB test datasets respectively.\\nIt should be noted this paper uses collaborative filtering based on dependency networks as a benchmark\\nmethod. The superiority of this method is discussed in [Breese et al., 1998, Heckerman et al., 2000]. In the\\nMS SQL Server 2000 data mining models, prediction is defined as a join operation [Microsoft, 2000] and is\\nthus optimized very well, taking less than 5 minutes to find the recommendations for the test datasets. Of\\ncourse, there is a need for post-processing to find the Top-5 o#ers. The specification for the Prediction Join\\ncan be found in [Microsoft, 2000].\\nFor many cases from the test data, we encountered that dependency network models returned NULL\\npredictions, meaning the recommender systems did not find any product to o#er some customers. The\\nfractions of the NULL predictions are 26.13%, 36.88%, and 41.47% for the WESTDB2, GENBUS, and\\nMSWeb datasets respectively - these null predictions were not considered as successes. The accuracy rates\\nreported above reflect this reality and a similar evaluation approach was used for e-VZpro.\\nThe results from our experiments with e-VZpro and association mining are reported in the following\\nsection. Several experiments were conducted by changing the support level (S) and confidence rating (C).\\nThe results indicated that e-VZpro is very competitive with the collaborative filtering system that uses the\\ndependency networks approach. An extension of e-VZpro and association mining that clustered the data\\nprior to the analysis also improved the results significantly.\\n\\nTable\\n\\n4: Accuracy Results from e-VZpro and association mining on WESTDB2 dataset\\ne-VZpro Assoc. Mining\\nSupport Confidence Number of Time Accuracy Time Accuracy\\nLevel\\n4.2 Implementing e-VZpro and Association Mining\\nThe current implementation of e-VZpro was designed to investigate the predictive ability of the method.\\nAs reported in [Karypis, 2001] we implemented e-VZpro and association mining in sparse matrix format,\\nkeeping the rules and customer profiles in sparse matrix format in memory. Customer profiles from the test\\ndatasets were read one at a time, which reduced the memory requirements for the experimental software,\\nbut also had some negative impact on the I/O times. As indicated in previous sections, association mining\\nhas been used successfully as a recommender system and experimental results for the traditional association\\nmining recommender system are reported.\\nThe results from the experiments with e-VZpro and association mining on the WESTDB2 and GEBUS\\ndatasets are show in Table 4 and Table 5, following. As previously mentioned, support level (S) and\\nconfidence rate (C) are important parameters in association mining. To investigate the e#ects of changing\\nthese factors, the association rules were generated according to 12 di#erent parameter sets. We used 3\\ndi#erent support levels (0.01%, 0.05%, and 0.1%) and 4 di#erent confidence rates (55%, 60%, 65%, and\\n75%). A corresponding number of rules are also reported in Tables 4 and 5 for the WESTDB2 and GENBUS\\ndatasets respectively. Decreasing the support level increases the number of rules and increasing the confidence\\nrate decreases the number of rules for any given support level. In a sense, support level and confidence rate\\nare used as two important knobs to adjust the overfitting or underfitting of e-VZpro.\\nThe running time of e-VZpro and association mining (in seconds) is reported in Table 4 and Table 5.\\nThe experiments were conducted on the same machine as dependency networks based recommender systems.\\nBecause computational times are dependent on the number of rules, the advantage of e-VZpro should be\\napparent with a smaller number of association rules. Note that it took only a few minutes to discover\\nthe association rules for any experiment in this study and the reported times do not include the discovery\\ntime. When the number of rules is less than the number of customers in the training data, e-VZpro is\\ncomputationally better than memory-based methods. For the best results, e-VZpro ended up using 26,850\\nand 92,389 rules on the WESTDB2 and GENBUS datasets, respectively; association mining takes less time\\nthan e-VZpro. The accuracy results are shown in Tables 4 and 5 and best performances are shown in bold.\\n\\nTable\\n\\n5: Accuracy Results from e-VZpro and association mining on GENBUS dataset\\ne-VZpro Assoc. Mining\\nSupport Confidence Number of Time Accuracy Time Accuracy\\nLevel\\ne-VZpro and association mining were run on the MSweb dataset using sing support and confidence levels.\\nWe arbitrarily chose 0.05% and 55% as support and confidence levels. e-VZpro and association mining used\\n30,181 rules to recommend pages for the MSWeb dataset. e-VZpro took 149 seconds to run and resulted in\\n50.51% accuracy while association mining took 115 seconds to and achieved 31.94% accuracy.\\nThese results clearly indicate that e-VZpro made consistently better predictions when compared to association\\nmining. For the WESTDB2 dataset, e-VZpro performed remarkably well (58.7% compared to\\n41.93%) and its best result, (46.20% compared to 47.59%) for the GENBUS dataset was very close to the result\\nof the dependency networks based recommender system discussed earlier. Note that e-VZpro performed\\nvery well for the less sparse dataset. At a fixed support level, increasing the confidence rate diminishes the\\naccuracy rates of e-VZpro in general; it can be concluded that e-VZpro can make better predictions when\\nits association rules are discovered at lower confidence rates. Changing confidence has a greater e#ect on\\nperformance of association mining than changing the support level and rules found at lower confidence levels\\nhave better predictive abilities.\\nBecause of e-VZpro\\'s design, it does not return NULL predictions. Association mining, however, resulted\\nin 12.86%, 38.98% and 41.01% NULL predictions for the WESTDB2, GENBUS, and MSWeb database\\nrespectively, pointing out a significant di#erence between e-VZpro and association mining.\\nWe can also easily determine that the support level used for the GENBUS dataset is not low enough.\\nIn other words, there are not su#cient association rules to represent the underlying relationships that exist\\nwithin the data. There are two approaches to handling this problem. We can either lower the support level\\nto generate a larger set of association rules, or we can cluster the training data and find the association rules\\nfor each of the clusters separately. This situation will definitely reduce the computational performance of\\ne-VZpro and the following subsection will investigate the e#ect of clustering on the predictive abilities of\\ne-VZpro and association mining.\\n\\nTable\\n\\nWESTDB2 GENBUS MSWeb\\n4.3 Using Clustering in Association Mining\\nAs noted above, to prevent the possibility of poor performance of e-VZpro and association mining in terms of\\naccuracy at low support levels and to investigate the clustering approach, we grouped the user-item matrix\\nof the datasets into 5 clusters. This was accomplished using the clustering algorithm provided by MS SQL\\nServer 2000 and we intentionally chose a small number of clusters to keep the experiment within manageable\\nlevels. For the sake of simplicity, we set the confidence rate to 55% for all datasets.\\nAs shown in Tables 4 and 5, e-VZpro and association mining take longer to run when there are too many\\nrules. In contrast, these methods perform poorly with very few rules but take a very short time to run.\\nBecause of the behavior of the clustering algorithm in MS SQL Server 2000, the first cluster always has the\\nhighest number of cases with later clusters containing less data. The last cluster, Cluster 5 in our case, has\\nthe lowest number of cases, but very dense data. In other words, the first cluster has the greatest number\\nof cases, but the fewest number of products included in customer profiles. Cluster 5, on the other hand, has\\na very small number of customers with highly dense profiles containing numerous products.\\nBecause of these discrepancies, it was necessary to set the support levels of the clustered data di#erently.\\nFor the WESTDB2 dataset, support levels were set to 0. 01%, 0.1%, 0.1%, 0.1%, and 0.1% respectively, for\\neach of the clusters from 1 to 5. These values were set to and 0.1% for each\\nof the clusters in the GENBUS dataset and 0.05%, 0.05%, 0.05%, 0.05%, and 0.1% for the MSWeb dataset.\\nAs seen from these support levels, higher levels were set for Cluster 5 in each dataset to prevent an excessive\\nnumber of association rules.\\nResults after running e-VZpro and association mining on clustered data are reported in Table 6. The\\naccuracy of e-VZpro and association mining with the number of test cases in each of the clusters is reported\\nin\\n\\nTable\\n\\n6 on WESTDB2, GENBUS, and MSWeb datasets. Overall accuracy of the e-VZpro has jumped\\nsignificantly on WESTDB2 (From 58.87% to 68.34%) and GENBUS (from 46.20% to 66.23%) after clustering\\nthe data.\\nThe results of association mining either improve or degrade a bit after clustering. Both e-VZpro and\\nassociation mining improve somewhat after clustering on the MSWeb dataset. We conclude that clustering\\ndata and then finding association rules for each cluster helps to improve e-VZpro significantly on the less\\nsparse data but has no e#ect on very sparse data. This point deserves greater investigation to determine if\\nclustering algorithms perform less e#ectively on data with a large number of dimensions. In short, e-VZpro\\nresulted in significantly better predictions than the dependency networks based recommender systems. In the\\nfollowing subsection we discuss the di#erences between the two methods and their e#ects on the corresponding\\n\\nTable\\n\\n7: Top-5 Recommendations for the Test Case Number\\nperformance of these two methods.\\n4.4 Further Discussions on e-VZpro and Dependency Networks\\nTo better depict the underlying methods, this section contains a test case from the WESTDB2 dataset. The\\ntest case number 3330 originally had Product 1, Product 26, Product 29, and Product 51 in the profile.\\nProduct 29 was randomly removed from the customer profile according to the all-but-1 protocol. Thus the\\nremaining products were used to find the Top-5 recommendations given in Table 7 for this customer profile.\\nIn\\n\\nTable\\n\\n7, the numbers on the left side and the right side of the hyphen correspond to the recommended\\nproduct and the probability (Score) of the prediction. Because both methods predicted Product 29 within\\nthe Top-5 recommendations, they are both successful for this test case. e-VZpro used the association rule\\nProduct 1, Product 26, Product 51 # Product 29 with a confidence rate of 0.6631. Since the rule and the\\ncustomer profile match exactly, the similarity between this particular profile and the rule is equal to 1. The\\nscore for this case then becomes 0.6631 for Product 29.\\nBased on the decision tree depicted in Figure 3, dependency networks predicted Product 29 as the top\\nrecommendation. From the Node Path window, we can see that the corresponding rule for this particular\\ncase is \"if Product 1 > 0.334 and Product 26 > 0.221 and Product 33 # 0.159 and Product 51 > 0.147 then\\nwith 0.7736 probability the case 0.606 (means 1) and 0.2248 probability the case is 0.106 (means 0)\". Since\\n0.7736 is greater than 0.5, the prediction is 1 for this particular case.\\nMany data mining and machine learning models might fail to perform well in certain cases due to the\\noverfitting or the underfitting problems. Recommender systems are no exception and changing the support\\nlevel and confidence rate in our approach controls the degree of the training process. Pruning is an important\\nstep in constructing decision trees to prevent overfitting. The decision tree model used in MS SQL Server\\n2000 does not allow the users to prune the trees, as the model is not flexible enough to allow the use of any\\npruning method. As it is seen from the unbalanced and very deep decision tree in Figure 3, the decision\\ntrees found in MS SQL Server 200 are highly overfitted. The only option we have in this case is to limit the\\nsample size, which explains why MS Commerce Server 2000 has an upper limit on the number of cases used\\nin training the recommender system. Overfitting could also be one of the reasons that recommender systems\\nbased on the dependency networks had poor results.\\nOverfitting in the dependency networks also caused the NULL prediction problem as was indicated in\\nSection 4.1. This resulted in a poor performance compared to the e-VZpro, which is designed not to predict\\nNULLs. However, higher NULL prediction rates in the dependency networks model indicate that the quality\\nof the recommendation is higher for regular recommendations.\\nOne should consider the di#erence between the product recommendation problem based on the historical\\npurchase data and on the customer ratings. Since training e-VZpro depends on the association mining, it is\\nnot clear how e-VZpro is used when ratings are present. Recommender systems based on the dependency\\nnetwork are very straightforward to use for these issues and show remarkable results [Breese et al., 1998,\\n\\nTable\\n\\n8: Results from Cosine, Correlation and Popularity Based Recommender Systems\\nWESTDB2 GENBUS MSWeb\\nMethod Accuracy Time Accuracy Time Accuracy Time\\nHeckerman et al., 2000].\\nThe biggest advantage of e-VZpro over dependency networks is the very short training times of e-VZpro.\\nPractically all the training time, e.g. discovering the association rules, of e-VZpro is negligible. However\\nthe prediction time of the dependency networks is very short due to their very small stored models. The\\noptimized nature of the Prediction Join operation in a relational database environment also contributes to\\nthese short prediction times [Microsoft, 2000]. Nevertheless, e-VZpro performed well in terms of computation\\ntime when using the sparse format.\\nExperiments were also run using other well-known recommender systems. To avoid excessive computational\\ntimes associated with the user-based methods, only item-based methodologies were used. In the\\nfollowing subsection, we report results from the simple, yet very powerful, item-based recommender systems\\nthat we tested. Our results confirm again that these methods are very robust and superior to others. The\\nalgorithm based on the cosine similarity performed the best or second best across all datasets.\\n4.5 Results from Other Recommender Systems\\nThis section contains the results from item-based algorithms using similarity measures based on cosine\\n(Equation 2) and Pearson\\'s correlation coe#cient (Equation 1) in this section. Of course, in item-based\\ncases similarities are computed between the items without changing the formulas given in Equations 1 and\\n2. This section also contains results from a popularity-based recommender system - while this last type of\\nsystem is the most nave, it is also very simple to implement and can be useful and powerful in certain cases.\\nEach of the methods described in this section is implemented in sparse format by adhering to the findings\\nin [Karypis, 2001]. We report accuracy and computational time in Table 8. Computational time includes\\nboth finding similarities between items and evaluating each of the test cases. Sparse implementation is very\\ne#ective in terms of the computational time as seen from the results.\\nAs indicated in [Sarwar et al., 2001], there are two steps involved in item-based algorithms:\\n. Similarity computations between items.\\n. Prediction.\\nThe prediction step is slightly di#erent in our implementation. As in [Sarwar et al., 2001], prediction\\nwas based on a weighted sum. But, to find the overall similarity between the items in the active customer\\nprofile and a given item, we used only the non-negative correlation coe#cients and discarded the negative\\ncorrelations. Thus, normalization of the overall similarity was accomplished by dividing the number of non-zero\\ncorrelation coe#cients between the active customer profile and a given item. For the cosine similarity\\nmeasure, no such change was needed, because these measures are between 0 and 1 for our datasets.\\nAs the results in Table 8 indicate, item-based recommender systems are very e#cient and robust in\\nterms of both scalability and prediction accuracy. Cosine based algorithms, especially, are superior to other\\nmethods mentioned in this paper.\\n5 Future Work and Conclusion\\nA thorough comparison of several recommender systems is presented in this paper. Experimental studies on\\nboth internal Verizon data and publicly available benchmark data were conducted using sparse binary data.\\nThe results of the experiments indicate that simple methods recommend products very accurately when\\ncompared to well-known recommender systems based on machine learning communities and dependency\\nnetworks introduced in [Breese et al., 1998, Heckerman et al., 2000]. We also proposed a new recommender\\nsystem, e-VZpro, that is based on association mining. Although the training time of e-VZpro is negligible\\nand it did provided e#ective predictions (which were improved further by the use of clustering), the current\\nimplementation does have the drawback of longer prediction times.\\nFor modeling purposes, we used customer profiles without a time dimension. Incorporating the time\\ninformation (i.e. purchase or service order times), the problem becomes one of sequence mining. This\\ntype of analysis may answer questions about what product/service the customer will purchase next and\\nrecommender systems using the two-step method of e-VZpro can be constructed to easily perform sequence\\nmining. One important benefit of this type of recommender system is that it can be used to predict potential\\nservice cancellations and o#er retention incentives to those at-risk customers.\\nA successful recommender system should also consider and model the acceptance and the decline rates\\nof the products. At the beginning of the deployment of any recommender system such modeling perspective\\nwould be problematic due to the unavailability of the historical acceptance and decline rates. Modeling\\nthe acceptance and the decline rates will allow showing the di#erent recommendations each time the customer\\ninteracts with the system. Recommender systems should also avoid from the NULL predictions. Na\\nrecommender systems such as the most frequent items should be used in this case.\\nA successful recommender system should also consider and model the acceptance and decline rates of\\nthe o#ered products. While the lack of historical acceptance/decline rates for newly deployed recommender\\nsystems can be problematic, modeling these rates will allow di#erent recommendations to be presented to\\nthe customer each time they interact with the system. NULL predictions should be avoided in recommender\\nsystems; in these cases, a more na-ve recommender system, such as a most frequent items system, should be\\nused.\\nThe most important consideration when deploying a commercial recommender system lies in determining\\nthe success metrics. Web-based recommender systems can track successes by measuring the clicks (conver-\\ngence rates) but recommender systems for e-commerce sites should also incorporate the click-stream data in\\norder to predict user needs and behavior to provide more accurate recommendations.\\n\\nAcknowledgements\\n\\nI would like to thank to Mohammed J. Zaki for his comments on an early draft of this paper, David\\nHeckerman for his directions regarding the implementation of the dependency networks, Syed Rafiuddin\\nand Wahib Omran for their valuable contributions in the preparation of the data, and numerous people in\\nE-Business Department for very productive discussions. Finally, I want to thank to Sam Witt for editing\\nthis paper.\\n\\n\\n\\n--R\\n\\nPartial classification using association rules.\\n\\n\\nUsing collaborative filtering to weave an information tapestry.\\nDependency networks for density estimation\\nThe UCI KDD archive.\\nEvaluation of item-based top-n recommendation algorithms\\nApplying collaborative filtering to usenet news.\\n\\n\\nRecommender systems.\\nPersonalized navigation for the web.\\n\\nAnalysis of recommender algorithms for e-commerce\\nElectronic commerce recommender applications.\\n\\n--TR\\n',\n",
       " 'gold': ['collaborative filtering',\n",
       "  'customer relationship management',\n",
       "  'e-commerce',\n",
       "  'recommender systems',\n",
       "  'dependency networks',\n",
       "  'association mining']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_DIR = Path(\"Krapivin2009\")\n",
    "\n",
    "def load_krapivin(dataset_dir: Path, max_docs=None):\n",
    "    \"\"\"\n",
    "    Load Krapivin2009 documents.\n",
    "    Returns a list of dicts {id, text, gold}.\n",
    "    max_docs allows loading only a subset (for testing).\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    text_files = sorted(dataset_dir.rglob(\"*.txt\"))\n",
    "\n",
    "    for i, text_path in enumerate(text_files):\n",
    "        if max_docs is not None and i >= max_docs:\n",
    "            break\n",
    "\n",
    "        doc_id = text_path.stem\n",
    "\n",
    "        # Look for the keywords file with the same name\n",
    "        key_path = None\n",
    "        for ext in (\".key\", \".uncontr\", \".keywords\"):\n",
    "            candidate = text_path.with_suffix(ext)\n",
    "            if candidate.exists():\n",
    "                key_path = candidate\n",
    "                break\n",
    "\n",
    "        # If not found nearby, search elsewhere in the folder\n",
    "        if key_path is None:\n",
    "            matches = list(dataset_dir.rglob(doc_id + \".key\"))\n",
    "            if matches:\n",
    "                key_path = matches[0]\n",
    "\n",
    "        if key_path is None:\n",
    "            # No gold for this doc, we ignore it\n",
    "            continue\n",
    "\n",
    "        text = text_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        gold = [\n",
    "            line.strip()\n",
    "            for line in key_path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            if line.strip()\n",
    "        ]\n",
    "\n",
    "        docs.append({\"id\": doc_id, \"text\": text, \"gold\": gold})\n",
    "\n",
    "    return docs\n",
    "\n",
    "# Load the dataset (adjust max_docs to go faster)\n",
    "# By default, there are 2304 documents\n",
    "docs = load_krapivin(DATASET_DIR, max_docs=30)\n",
    "len(docs)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d9097",
   "metadata": {},
   "source": [
    "3. Apply 3 keyphrase extraction algorithms for each document in the dataset.\n",
    "We chose the following algorithms:\n",
    "- TfIdf (statistical method)\n",
    "- TextRank (graph-based + PageRank)\n",
    "- KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d22882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pke\n",
    "\n",
    "# Make sure English stopwords are available\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "STOPLIST = stopwords.words(\"english\") + list(string.punctuation)\n",
    "POS_TAGS = {\"NOUN\", \"PROPN\", \"ADJ\"}\n",
    "\n",
    "\n",
    "def extract_kp(model_cls, text, n=10):\n",
    "    \"\"\"\n",
    "    Apply a pke model (TfIdf, TextRank...)\n",
    "    on a text and return the n best keyphrases.\n",
    "    \"\"\"\n",
    "    extractor = model_cls()\n",
    "\n",
    "    # Special case for TfIdf\n",
    "    if model_cls is pke.unsupervised.TfIdf:\n",
    "        extractor.load_document(\n",
    "            input=text,\n",
    "            language=\"en\",\n",
    "            stoplist=STOPLIST,\n",
    "            normalization=None,\n",
    "        )\n",
    "        extractor.candidate_selection(n=3)\n",
    "        extractor.candidate_weighting()\n",
    "\n",
    "    # TextRank (word graph)\n",
    "    elif model_cls is pke.unsupervised.TextRank:\n",
    "        extractor.load_document(\n",
    "            input=text,\n",
    "            language=\"en\",\n",
    "            normalization=None,\n",
    "        )\n",
    "        extractor.candidate_selection(pos=POS_TAGS)\n",
    "        extractor.candidate_weighting()\n",
    "\n",
    "    # Generic fallback for other pke models\n",
    "    else:\n",
    "        extractor.load_document(\n",
    "            input=text,\n",
    "            language=\"en\",\n",
    "            normalization=None,\n",
    "        )\n",
    "        extractor.candidate_selection()\n",
    "        extractor.candidate_weighting()\n",
    "\n",
    "    return [kp for kp, score in extractor.get_n_best(n=n)]\n",
    "\n",
    "\n",
    "def run_model_on_corpus(model_cls, field_name, n=10):\n",
    "    start = time.time()\n",
    "    for doc in docs:\n",
    "        doc[field_name] = extract_kp(model_cls, doc[\"text\"], n=n)\n",
    "    duration = time.time() - start\n",
    "    print(\n",
    "        f\"{field_name}: {len(docs)} docs processed, \"\n",
    "        f\"{duration/len(docs):.3f} s/doc on average\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the 2 algorithms (top-10 keyphrases)\n",
    "# run_model_on_corpus(pke.unsupervised.TfIdf,    \"pred_tfidf\",    n=10)\n",
    "# run_model_on_corpus(pke.unsupervised.TextRank, \"pred_textrank\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f627a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")  # lightweight model\n",
    "\n",
    "def extract_kp_keybert(text, n=10):\n",
    "    kps = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 3),\n",
    "        stop_words=\"english\",\n",
    "        top_n=n\n",
    "    )\n",
    "    return [kp for kp, score in kps]\n",
    "\n",
    "# for doc in docs:\n",
    "#     doc[\"pred_keybert\"] = extract_kp_keybert(doc[\"text\"], n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cb1e81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:LoadFile._df_counts is hard coded to /home/nathan/miniconda3/lib/python3.13/site-packages/pke/models/df-semeval2010.tsv.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 docs processed in parallel (8.492 s/doc on average, 28 workers)\n",
      "TF-IDF: 101.4966 s/doc\n",
      "TextRank: 30.2904 s/doc\n",
      "KeyBERT: 93.5499 s/doc\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def process_doc_all_methods(doc, n=10):\n",
    "    \"\"\"\n",
    "    Applique les trois mthodes (TfIdf, TextRank, KeyBERT)\n",
    "    sur un document et renvoie les listes de keyphrases.\n",
    "    \"\"\"\n",
    "    text = doc[\"text\"]\n",
    "    timings = {}\n",
    "\n",
    "    # TF-IDF\n",
    "    start_tfidf = time.time()\n",
    "    pred_tfidf = extract_kp(pke.unsupervised.TfIdf, text, n=n)\n",
    "    timings[\"tfidf\"] = time.time() - start_tfidf\n",
    "\n",
    "    # TextRank\n",
    "    start_textrank = time.time()\n",
    "    pred_textrank = extract_kp(pke.unsupervised.TextRank, text, n=n)\n",
    "    timings[\"textrank\"] = time.time() - start_textrank\n",
    "\n",
    "    # KeyBERT\n",
    "    start_keybert = time.time()\n",
    "    pred_keybert = extract_kp_keybert(text, n=n)\n",
    "    timings[\"keybert\"] = time.time() - start_keybert\n",
    "\n",
    "    return doc[\"id\"], pred_tfidf, pred_textrank, pred_keybert, timings\n",
    "\n",
    "\n",
    "def run_all_methods_in_parallel(docs, n=10, max_workers=None):\n",
    "    \"\"\"\n",
    "    Lance le traitement de tous les documents en parallle,\n",
    "    en utilisant jusqu' max_workers threads CPU.\n",
    "    Pour chaque doc, on calcule pred_tfidf, pred_textrank, pred_keybert.\n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = os.cpu_count() or 4\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Pour retrouver les docs facilement par id\n",
    "    id2doc = {doc[\"id\"]: doc for doc in docs}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [\n",
    "            executor.submit(process_doc_all_methods, doc, n)\n",
    "            for doc in docs\n",
    "        ]\n",
    "\n",
    "        all_timings = []\n",
    "\n",
    "        for fut in as_completed(futures):\n",
    "            doc_id, pred_tfidf, pred_textrank, pred_keybert, timings = fut.result()\n",
    "            d = id2doc[doc_id]\n",
    "            d[\"pred_tfidf\"]    = pred_tfidf\n",
    "            d[\"pred_textrank\"] = pred_textrank\n",
    "            d[\"pred_keybert\"]  = pred_keybert\n",
    "            all_timings.append(timings)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    print(\n",
    "        f\"{len(docs)} docs processed in parallel \"\n",
    "        f\"({duration/len(docs):.3f} s/doc on average, \"\n",
    "        f\"{max_workers} workers)\"\n",
    "    )\n",
    "    tfidf_times = [t[\"tfidf\"] for t in all_timings]\n",
    "    textrank_times = [t[\"textrank\"] for t in all_timings]\n",
    "    keybert_times = [t[\"keybert\"] for t in all_timings]\n",
    "\n",
    "    print(f\"TF-IDF: {np.mean(tfidf_times):.4f} s/doc\")\n",
    "    print(f\"TextRank: {np.mean(textrank_times):.4f} s/doc\")\n",
    "    print(f\"KeyBERT: {np.mean(keybert_times):.4f} s/doc\")\n",
    "\n",
    "\n",
    "# Lancer le traitement parallle (top-10 keyphrases)\n",
    "run_all_methods_in_parallel(docs, n=10, max_workers=28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20338238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export 29 documents dans /home/nathan/Documents/GitHub/KeyPhraseExtraction/krapivin_kpe_results.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '1005058',\n",
       " 'gold': ['collaborative filtering',\n",
       "  'customer relationship management',\n",
       "  'e-commerce',\n",
       "  'recommender systems',\n",
       "  'dependency networks',\n",
       "  'association mining'],\n",
       " 'pred_tfidf': ['recommender',\n",
       "  'customer',\n",
       "  'systems',\n",
       "  'association',\n",
       "  'used',\n",
       "  'e-vzpro',\n",
       "  'recommender systems',\n",
       "  'mining',\n",
       "  'rules',\n",
       "  'dependency'],\n",
       " 'pred_textrank': ['enhancing product recommender systems',\n",
       "  'historical customer data',\n",
       "  'customer historical data',\n",
       "  'customer purchases product a',\n",
       "  'customer profile data',\n",
       "  'sample customer data',\n",
       "  'customer purchase data',\n",
       "  'other item-based recommender systems',\n",
       "  'method accuracy time accuracy time accuracy time',\n",
       "  'other model-based recommender systems'],\n",
       " 'pred_keybert': ['association mining recommender',\n",
       "  'mining based recommender',\n",
       "  'mining recommender',\n",
       "  'recommender frequent items',\n",
       "  'collaborative filtering technique',\n",
       "  'items products recommender',\n",
       "  'collaborative filtering algorithm',\n",
       "  'recommender tool designed',\n",
       "  'item based recommender',\n",
       "  'collaborative filtering algorithms']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Dossier et nom du fichier de sortie\n",
    "OUTPUT_PATH = Path(\"krapivin_kpe_results.json\")\n",
    "\n",
    "export_data = []\n",
    "\n",
    "for doc in docs:\n",
    "    export_data.append(\n",
    "        {\n",
    "            \"id\": doc[\"id\"],\n",
    "            # mots-cls de rfrence (auteur)\n",
    "            \"gold\": doc.get(\"gold\", []),\n",
    "            # keyphrases prdites par chaque mthode\n",
    "            \"pred_tfidf\": doc.get(\"pred_tfidf\", []),\n",
    "            \"pred_textrank\": doc.get(\"pred_textrank\", []),\n",
    "            \"pred_keybert\": doc.get(\"pred_keybert\", []),\n",
    "        }\n",
    "    )\n",
    "\n",
    "with OUTPUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(export_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Export {len(export_data)} documents dans {OUTPUT_PATH.resolve()}\")\n",
    "\n",
    "# Afficher un exemple pour vrifier\n",
    "export_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100ceacc",
   "metadata": {},
   "source": [
    "4. Evaluate with ROUGE and BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c9800cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = []\n",
    "cands_tfidf = []\n",
    "cands_textrank = []\n",
    "cands_keybert = []\n",
    "\n",
    "for doc in docs:\n",
    "    refs.append(\", \".join(doc[\"gold\"]))\n",
    "    cands_tfidf.append(\", \".join(doc[\"pred_tfidf\"]))\n",
    "    cands_textrank.append(\", \".join(doc[\"pred_textrank\"]))\n",
    "    cands_keybert.append(\", \".join(doc[\"pred_keybert\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c31081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L\n",
      "  TfIdf   : 0.2131\n",
      "  TextRank: 0.1144\n",
      "  KeyBERT : 0.1681\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "def eval_model(cands):\n",
    "    scores = []\n",
    "    for ref, cand in zip(refs, cands):\n",
    "        scores.append(scorer.score(ref, cand)[\"rougeL\"].fmeasure)\n",
    "    return sum(scores) / len(scores)\n",
    "\n",
    "rouge_tfidf    = eval_model(cands_tfidf)\n",
    "rouge_textrank = eval_model(cands_textrank)\n",
    "rouge_keybert  = eval_model(cands_keybert)\n",
    "\n",
    "print(\"Average ROUGE-L\")\n",
    "print(f\"  TfIdf   : {rouge_tfidf:.4f}\")\n",
    "print(f\"  TextRank: {rouge_textrank:.4f}\")\n",
    "print(f\"  KeyBERT : {rouge_keybert:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "168a5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 BERTScore\n",
      "  TfIdf   : 0.8643\n",
      "  TextRank: 0.8293\n",
      "  KeyBERT : 0.8430\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score as bert_score\n",
    "\n",
    "def avg_bertscore(cands, refs):\n",
    "    P, R, F1 = bert_score(cands, refs, lang=\"en\")\n",
    "    return float(F1.mean())\n",
    "\n",
    "bert_tfidf    = avg_bertscore(cands_tfidf, refs)\n",
    "bert_textrank = avg_bertscore(cands_textrank, refs)\n",
    "bert_keybert  = avg_bertscore(cands_keybert, refs)\n",
    "\n",
    "print(\"Average F1 BERTScore\")\n",
    "print(f\"  TfIdf   : {bert_tfidf:.4f}\")\n",
    "print(f\"  TextRank: {bert_textrank:.4f}\")\n",
    "print(f\"  KeyBERT : {bert_keybert:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
